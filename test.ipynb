{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrieval_tool import ContextGrabber\n",
    "import tiktoken\n",
    "from graph_parser import SimpleASTGraphBuilder\n",
    "from indexer import SimpleRepoIndexer\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def wrapper(issue_title,issue_metadata,url):\n",
    "    #build documents.json\n",
    "    if not os.path.exists(\"documents.json\"):\n",
    "        token_counter = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        indexer = SimpleRepoIndexer(\n",
    "            url, token_counter\n",
    "        )\n",
    "        indexer.index_repo()\n",
    "    if not os.path.exists(\"graph.gml\"):\n",
    "        token_counter = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "        graph_builder = SimpleASTGraphBuilder(url, token_counter)\n",
    "        graph_builder.build_graph(\"graph.gml\")\n",
    "\n",
    "    cg = ContextGrabber(\"graph.gml\",\"documents.json\")\n",
    "\n",
    "    response, relevant_chunks, helpful_chunks = cg.workflow(issue_title,issue_metadata)\n",
    "    return response, relevant_chunks, helpful_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/spcl/graph-of-thoughts\"\n",
    "issue_title = \"Get response and input token costs from OpenAI website via a webcrawler, using the model id.\"\n",
    "issue_metadata = \"prompt_token_cost, response_token_cost, model_id, ChatGPT(AbstractLanguageModel)\"\n",
    "response, relevant_chunks, helpful_chunks = wrapper(issue_title,issue_metadata,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Task Ticket: Retrieve response and input token costs from OpenAI website using a webcrawler\n",
      "\n",
      "## Description\n",
      "Implement a webcrawler that retrieves the response and input token costs from the OpenAI website using the model ID. The webcrawler should be able to extract the costs for different operations and store them for further analysis.\n",
      "\n",
      "## Requirements\n",
      "1. The webcrawler should be able to navigate to the OpenAI website and retrieve the response and input token costs for a given model ID.\n",
      "2. The webcrawler should be able to handle different operations and extract the costs associated with each operation.\n",
      "3. The extracted costs should be stored in a suitable data structure for further analysis.\n",
      "4. The webcrawler should be implemented as a class with the following methods:\n",
      "   - `_execute(lm: AbstractLanguageModel, prompter: Prompter, parser: Parser, **kwargs) -> None`: This method should handle the actual execution of the webcrawler. It should take the language model, prompter, parser, and any additional parameters as input.\n",
      "   - `get_thoughts() -> List[Thought]`: This method should retrieve the thoughts associated with the webcrawler operation.\n",
      "   - `parse_score_answer(states: List[Dict], texts: List[str]) -> List[float]`: This method should parse the response from the language model for a score prompt and return the scores for the thought states.\n",
      "   - `parse_aggregation_answer(states: List[Dict], texts: List[str]) -> Union[Dict, List[Dict]]`: This method should parse the response from the language model for an aggregation prompt and return the new thought states after parsing.\n",
      "   - `parse_improve_answer(state: Dict, texts: List[str]) -> Dict`: This method should parse the response from the language model for an improve prompt and return the new thought state after parsing.\n",
      "\n",
      "## Subtasks\n",
      "\n",
      "1. **Implement webcrawler class** - Implement the webcrawler class with the required methods and attributes. The class should be able to navigate to the OpenAI website and retrieve the response and input token costs for a given model ID. The class should also be able to handle different operations and extract the costs associated with each operation. *(Priority: 10)*\n",
      "\n",
      "2. **Implement `_execute` method** - Implement the `_execute` method in the webcrawler class. This method should handle the actual execution of the webcrawler. It should take the language model, prompter, parser, and any additional parameters as input. *(Priority: 9)*\n",
      "\n",
      "3. **Implement `get_thoughts` method** - Implement the `get_thoughts` method in the webcrawler class. This method should retrieve the thoughts associated with the webcrawler operation. *(Priority: 8)*\n",
      "\n",
      "4. **Implement `parse_score_answer` method** - Implement the `parse_score_answer` method in the webcrawler class. This method should parse the response from the language model for a score prompt and return the scores for the thought states. *(Priority: 7)*\n",
      "\n",
      "5. **Implement `parse_aggregation_answer` method** - Implement the `parse_aggregation_answer` method in the webcrawler class. This method should parse the response from the language model for an aggregation prompt and return the new thought states after parsing. *(Priority: 6)*\n",
      "\n",
      "6. **Implement `parse_improve_answer` method** - Implement the `parse_improve_answer` method in the webcrawler class. This method should parse the response from the language model for an improve prompt and return the new thought state after parsing. *(Priority: 5)*\n",
      "\n",
      "7. **Instantiate webcrawler class** - Instantiate the webcrawler class with the necessary parameters. *(Priority: 4)*\n",
      "\n",
      "8. **Call `_execute` method** - Call the `_execute` method of the webcrawler class to start the webcrawler and retrieve the response and input token costs. *(Priority: 3)*\n",
      "\n",
      "9. **Use `get_thoughts` method** - Use the `get_thoughts` method of the webcrawler class to retrieve the thoughts associated with the webcrawler operation. *(Priority: 2)*\n",
      "\n",
      "10. **Use `parse_score_answer`, `parse_aggregation_answer`, and `parse_improve_answer` methods** - Use the `parse_score_answer`, `parse_aggregation_answer`, and `parse_improve_answer` methods of the webcrawler class to parse the responses from the language model. *(Priority: 1)*\n",
      "\n",
      "## Code Snippets\n",
      "```python\n",
      "@abstractmethod\n",
      "def generate_prompt(self, num_branches: int, **kwargs) -> str:\n",
      "    \"\"\"\n",
      "    Generate a generate prompt for the language model.\n",
      "    The thought state is unpacked to allow for additional keyword arguments\n",
      "    and concrete implementations to specify required arguments explicitly.\n",
      "\n",
      "    :param num_branches: The number of responses the prompt should ask the LM to generate.\n",
      "    :type num_branches: int\n",
      "    :param kwargs: Additional keyword arguments.\n",
      "    :return: The generate prompt.\n",
      "    :rtype: str\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def validation_prompt(self, **kwargs) -> str:\n",
      "    \"\"\"\n",
      "    Generate a validation prompt for the language model.\n",
      "    The thought state is unpacked to allow for additional keyword arguments\n",
      "    and concrete implementations to specify required arguments explicitly.\n",
      "\n",
      "    :param kwargs: Additional keyword arguments.\n",
      "    :return: The validation prompt.\n",
      "    :rtype: str\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def score_prompt(self, state_dicts: List[Dict], **kwargs) -> str:\n",
      "    \"\"\"\n",
      "    Generate a score prompt for the language model.\n",
      "\n",
      "    :param state_dicts: The thought states that should be scored,\n",
      "                        if more than one, they should be scored together.\n",
      "    :type state_dicts: List[Dict]\n",
      "    :param kwargs: Additional keyword arguments.\n",
      "    :return: The score prompt.\n",
      "    :rtype: str\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def _execute(\n",
      "    self, lm: AbstractLanguageModel, prompter: Prompter, parser: Parser, **kwargs\n",
      ") -> None:\n",
      "    \"\"\"\n",
      "    Abstract method for the actual execution of the operation.\n",
      "    This should be implemented in derived classes.\n",
      "\n",
      "    :param lm: The language model to be used.\n",
      "    :type lm: AbstractLanguageModel\n",
      "    :param prompter: The prompter for crafting prompts.\n",
      "    :type prompter: Prompter\n",
      "    :param parser: The parser for parsing responses.\n",
      "    :type parser: Parser\n",
      "    :param kwargs: Additional parameters for execution.\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def get_thoughts(self) -> List[Thought]:\n",
      "    \"\"\"\n",
      "    Abstract method to retrieve the thoughts associated with the operation.\n",
      "    This should be implemented in derived classes.\n",
      "\n",
      "    :return: List of associated thoughts.\n",
      "    :rtype: List[Thought]\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def parse_aggregation_answer(\n",
      "    self, states: List[Dict], texts: List[str]\n",
      ") -> Union[Dict, List[Dict]]:\n",
      "    \"\"\"\n",
      "    Parse the response from the language model for a aggregation prompt.\n",
      "\n",
      "    :param states: The thought states used to generate the prompt.\n",
      "    :type states: List[Dict]\n",
      "    :param texts: The responses to the prompt from the language model.\n",
      "    :type texts: List[str]\n",
      "    :return: The new thought states after parsing the response from the language model.\n",
      "    :rtype: Union[Dict, List[Dict]]\n",
      "    \"\"\"\n",
      "    pass\n",
      "\n",
      "@abstractmethod\n",
      "def parse_improve_answer(self, state: Dict, texts: List[str]) -> Dict:\n",
      "    \"\"\"\n",
      "    Parse the response from the language model for an improve prompt.\n",
      "\n",
      "    :param state: The thought state used to generate the prompt.\n",
      "    :type state: Dict\n",
      "    :param texts: The responses to the prompt from the language model.\n",
      "    :type texts: List[str]\n",
      "    :return: The new thought state after parsing the response from the language model.\n",
      "    :rtype: Dict\n",
      "    \"\"\"\n",
      "    pass\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "time_taken = 24.3\n",
    "output_json = {\n",
    "    \"response\": response,\n",
    "    \"relevant_chunks\": relevant_chunks,\n",
    "    \"helpful_chunks\": helpful_chunks,\n",
    "    \"time_taken\": time_taken\n",
    "}\n",
    "with open(\"output.json\",\"w\") as f:\n",
    "    f.write(json.dumps(output_json,indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arnold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
